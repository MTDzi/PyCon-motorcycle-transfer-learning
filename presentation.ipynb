{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](img/nethone_logo_full_black.png)\n",
    "![](img/daftcode_logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, let's look at the bottleneck activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from utils import read_bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DF_activations = read_bottlenecks(model_name='mobilenet_1.0_224')\n",
    "\n",
    "unique_labels = DF_activations['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use **t-SNE** (t-distributed Stochastic Neighbor Embedding) for dimensionality reduction, to see whether the activation patterns in the last hidden layer (here called \"bottleneck\") can be grouped into clouds of points that (hopefully) correspond to the motorcycle types we're trying to identify.\n",
    "\n",
    "(Note that *scikit-learn*'s TSNE by default uses the the Barnes-Hut approximation, which is much faster, but may lead to sub-optimal embeddings.)\n",
    "\n",
    "Dimesionality reduction is a branch of the unsupervised machine learning field, which means that it does not look at the labels of the points (types of motorcycles). We'll use the labels only to color the points, after TSNE finds a mapping to the 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, init='pca', random_state=0, learning_rate=1)\n",
    "DF_tsne_2D = pd.DataFrame(\n",
    "    tsne.fit_transform(DF_activations.select_dtypes(include=[pd.np.number]))\n",
    ")\n",
    "DF_tsne_2D['label'] = DF_activations['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.rc_context(dict(sns.axes_style(\"darkgrid\"),\n",
    "                         **sns.plotting_context(\"notebook\", font_scale=2.2))):\n",
    "    fg = sns.FacetGrid(\n",
    "        data=DF_tsne_2D,\n",
    "        hue='label',\n",
    "        hue_order=unique_labels,\n",
    "        aspect=1.3,\n",
    "        size=10,)\n",
    "    fg.map(plt.scatter, 0, 1, s=120).add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we'll see predictions for not yet seen images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we're using the `create_model_info` and `add_jpeg_decoding` functions from the \"retrain.py\" script. The first returns a dictionary describing the architecture (mainly tensor names that we then pass to `sess.graph.get_tensor_by_name`). The second function returns two tensors -- `jpeg_data_tensor` and `decoded_image_tensor` -- that correspond to: the raw string of the JPEG file, and to a resized, preprocessed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from utils import load_graph\n",
    "sys.path.append('tensorflow/tensorflow/examples/image_retraining/')\n",
    "from retrain import create_model_info, add_jpeg_decoding\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "\n",
    "test_filenames = sorted(glob.glob('./test_images/*'))\n",
    "with open('labels.txt') as labels_file:\n",
    "    labels = [line.strip() for line in labels_file.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this presentation we'll only see predictions made by the \"mobilenet_1.0_224\" model, but you can carry out your own analysis by switching the `architecture` to \"inception_v3\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "architecture = 'mobilenet_1.0_224' # 'inception_v3'\n",
    "tf.reset_default_graph()\n",
    "load_graph(architecture)\n",
    "\n",
    "model_info = create_model_info(architecture)\n",
    "jpeg_data_tensor, decoded_image_tensor = add_jpeg_decoding(\n",
    "        model_info['input_width'], model_info['input_height'],\n",
    "        model_info['input_depth'], model_info['input_mean'],\n",
    "        model_info['input_std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to create a TensorFlow session, and to calculate the values of the tensors for particular input data (raw JPG strings), corresponding to images in the \"test_images/\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_layer_name = 'final_result:0'\n",
    "resized_input_tensor_name = model_info['resized_input_tensor_name']\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    softmax_tensor = sess.graph.get_tensor_by_name(output_layer_name)\n",
    "    resized_input_tensor = sess.graph.get_tensor_by_name(resized_input_tensor_name)\n",
    "\n",
    "    for filename in test_filenames:\n",
    "        with open(filename, 'rb') as image_file:\n",
    "            image_data = image_file.read()\n",
    "\n",
    "        image = sess.run(decoded_image_tensor, \n",
    "                         {jpeg_data_tensor: image_data})\n",
    "        predictions, = sess.run(softmax_tensor, \n",
    "                                {resized_input_tensor: image})\n",
    "\n",
    "        print('\\n\\n\\n{}'.format(filename))\n",
    "        display(Image(filename, width=1000))\n",
    "        print('')\n",
    "\n",
    "        top_k = predictions.argsort()[::-1]\n",
    "        for node_id in top_k:\n",
    "            human_string = labels[node_id]\n",
    "            score = 100*predictions[node_id]\n",
    "            print('{:6.2f}% for {}'.format(score, human_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how a preprocessed image looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(image[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the image is \"seen\" by the model.\n",
    "\n",
    "Some of the images were misclassified, we'll gather them into a list `weird_cases`, and perform an analysis indicating which parts of the image were relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weird_cases = ['./test_images/cruiser1.jpeg', \n",
    "               './test_images/x2.jpeg',\n",
    "               './test_images/superbike3.jpeg',\n",
    "               './test_images/cross1.jpeg',\n",
    "               './test_images/cross2.jpeg',\n",
    "               './test_images/classic2.jpeg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which parts of the images led the model to these predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we're using the LIME package to visualize which patches (superpixels) were relevant for the model when predicting motorcycle class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "from utils import GraphWrap\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for weird_case in weird_cases:\n",
    "        with open(weird_case, 'rb') as image_file:\n",
    "                image_data = image_file.read()\n",
    "        print('\\n\\n\\n### IMAGE: \"{}\" ##################################################'\n",
    "              .format(weird_case))\n",
    "        \n",
    "        wrapper = GraphWrap(sess, resized_input_tensor_name)\n",
    "        image = sess.run(decoded_image_tensor, {jpeg_data_tensor: image_data})\n",
    "        image = image[0]\n",
    "        prediction = wrapper.predict([image])[0]\n",
    "\n",
    "        explainer = lime_image.LimeImageExplainer()\n",
    "        explanation = explainer.explain_instance(image,\n",
    "                                                 wrapper.predict,\n",
    "                                                 top_labels=5,\n",
    "                                                 num_samples=1000)\n",
    "        \n",
    "        for k in range(len(labels)):\n",
    "            print('\\n\\n\\n{:6.2f}% for {}'.format(100*prediction[k], labels[k]))\n",
    "            img, mask = explanation.get_image_and_mask(k,\n",
    "                                                        positive_only=False,\n",
    "                                                        num_features=5,\n",
    "                                                        hide_rest=False)\n",
    "            plt.imshow(mark_boundaries(img, mask))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll find the materials for this presentation at: https://github.com/daftcode/PyCon-motorcycle-transfer-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General references\n",
    "\n",
    "A valuable set of rules regarding transfer learning: http://cs231n.github.io/transfer-learning/\n",
    "\n",
    "MobileNets on Google Research Blog: https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html\n",
    "\n",
    "A great source of learning materials: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/\n",
    "\n",
    "The LIME repo: https://github.com/marcotcr/lime/ (where you'll also find a link to their article describing the algorithm in more detail)\n",
    "\n",
    "\n",
    "### More detailed sources\n",
    "\n",
    "An interesing Issue in the TensorFlow repo (in particular, there's an valuable observation regarding how resizing/cropping of images influences predictions): https://github.com/tensorflow/tensorflow/issues/4128\n",
    "\n",
    "\"Inception module: explained and implemented\": https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/\n",
    "\n",
    "Two softmax outputs in the Inception v3 model?: https://stackoverflow.com/questions/39352108/does-the-inception-model-have-two-softmax-outputs\n",
    "\n",
    "How to integrate MobileNets into your project: https://github.com/tensorflow/models/blob/master/slim/README.md\n",
    "\n",
    "\n",
    "### Other interesting resources\n",
    "\n",
    "TensorFlow mobile: https://www.tensorflow.org/mobile/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
